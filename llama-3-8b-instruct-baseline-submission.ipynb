{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe8c2bfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T18:21:14.841601Z",
     "iopub.status.busy": "2024-04-20T18:21:14.840943Z",
     "iopub.status.idle": "2024-04-20T18:21:15.780784Z",
     "shell.execute_reply": "2024-04-20T18:21:15.779910Z"
    },
    "papermill": {
     "duration": 0.946027,
     "end_time": "2024-04-20T18:21:15.782827",
     "exception": false,
     "start_time": "2024-04-20T18:21:14.836800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning-agency-lab-automated-essay-scoring-2  llama-3-8b-lora-fine-tuned-exp-1\r\n",
      "llama-3-8b-instruct\r\n"
     ]
    }
   ],
   "source": [
    "!ls /kaggle/input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d22970d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T18:21:15.789475Z",
     "iopub.status.busy": "2024-04-20T18:21:15.789180Z",
     "iopub.status.idle": "2024-04-20T18:21:15.797170Z",
     "shell.execute_reply": "2024-04-20T18:21:15.796270Z"
    },
    "papermill": {
     "duration": 0.013819,
     "end_time": "2024-04-20T18:21:15.799245",
     "exception": false,
     "start_time": "2024-04-20T18:21:15.785426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing infer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile infer.py\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from peft import PeftModel\n",
    "from types import SimpleNamespace\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "\n",
    "def main(args):\n",
    "    config = SimpleNamespace(\n",
    "        data_dir = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2',\n",
    "    )\n",
    "\n",
    "    model     = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_pth,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model     = PeftModel.from_pretrained(model, args.lora_pth)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_pth, padding_side='right') \n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    def preprocess(sample, text=False, infer_mode=False, max_seq=args.max_length, return_tensors=None):\n",
    "        sys_prompt = \"Please read the following essay and assign a score of 1,2,3,4,5,6 where 6 is the best. Output only a single number with no explanation.\\n\\n\"\n",
    "        prompt = sample[\"full_text\"]\n",
    "        if infer_mode: answer = \"\"\n",
    "        else: answer = str(sample[\"score\"])\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": sys_prompt + prompt},\n",
    "            {\"role\": \"assistant\", \"content\": f\"\\n\\nThe score is: \" + answer}\n",
    "        ]\n",
    "        formatted_sample = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        if infer_mode: formatted_sample = formatted_sample.replace(\"<|eot_id|>\",\"\")\n",
    "\n",
    "        tokenized_sample = tokenizer(formatted_sample, padding=True, return_tensors=return_tensors, \n",
    "                                     truncation=True, add_special_tokens=False, max_length=max_seq) \n",
    "\n",
    "        if return_tensors==\"pt\":\n",
    "            tokenized_sample[\"labels\"] = tokenized_sample[\"input_ids\"].clone()\n",
    "        else:\n",
    "            tokenized_sample[\"labels\"] = tokenized_sample[\"input_ids\"].copy()\n",
    "\n",
    "        if text: return formatted_sample\n",
    "        else: return tokenized_sample\n",
    "    \n",
    "    df_test = pd.read_csv(f'{config.data_dir}/test.csv')\n",
    "    sub     = pd.read_csv(f'{config.data_dir}/sample_submission.csv')\n",
    "    \n",
    "    test_preds = []\n",
    "\n",
    "    for i,row in tqdm(df_test.iterrows(), total=len(df_test)):\n",
    "\n",
    "        tokenized_sample = preprocess(row, infer_mode=True, max_seq=args.max_length, return_tensors=\"pt\")\n",
    "        generated_ids = model.generate(**tokenized_sample.to('cuda'), max_new_tokens=2,\n",
    "                                       pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "        decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        try:\n",
    "            answer = decoded[0].rsplit(\"The score is: \", 1)[1]\n",
    "            test_preds.append( int(answer) )\n",
    "        except:\n",
    "            test_preds.append( 3 )\n",
    "            \n",
    "    sub.score = test_preds\n",
    "    sub.score = sub.score.astype('int')\n",
    "    sub.to_csv(args.sub_pth, index=False)\n",
    "    \n",
    "    del model, tokenizer\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_pth\",  type=str, required=True, help=\"Path to the pretrained model\" )\n",
    "    parser.add_argument(\"--lora_pth\",   type=str, required=True, help=\"Path to the PEFT LoRA adapter\")\n",
    "    parser.add_argument(\"--sub_pth\",    type=str, required=True, help=\"Path to save submission file\" )\n",
    "    parser.add_argument(\"--max_length\", type=int, required=True, help=\"Max length of input sequence\" )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd824871",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-04-20T18:21:15.805094Z",
     "iopub.status.busy": "2024-04-20T18:21:15.804815Z",
     "iopub.status.idle": "2024-04-20T18:25:24.717226Z",
     "shell.execute_reply": "2024-04-20T18:25:24.716252Z"
    },
    "papermill": {
     "duration": 248.917889,
     "end_time": "2024-04-20T18:25:24.719502",
     "exception": false,
     "start_time": "2024-04-20T18:21:15.801613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [03:29<00:00, 52.31s/it]\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]2024-04-20 18:25:03.951941: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-04-20 18:25:03.952053: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-04-20 18:25:04.052936: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:23<00:00,  7.86s/it]\r\n"
     ]
    }
   ],
   "source": [
    "!python infer.py \\\n",
    "    --max_length 2048 \\\n",
    "    --sub_pth submission.csv \\\n",
    "    --model_pth /kaggle/input/llama-3-8b-instruct/Meta-Llama-3-8B-Instruct \\\n",
    "    --lora_pth /kaggle/input/llama-3-8b-lora-fine-tuned-exp-1/Meta-Llama-3-8B-Instruct-max-len-1024-fold-1-exp-1-ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17377fa",
   "metadata": {
    "papermill": {
     "duration": 0.003073,
     "end_time": "2024-04-20T18:25:24.726113",
     "exception": false,
     "start_time": "2024-04-20T18:25:24.723040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8059942,
     "sourceId": 71485,
     "sourceType": "competition"
    },
    {
     "sourceId": 173056568,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 173052744,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 173052860,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 252.787927,
   "end_time": "2024-04-20T18:25:24.948322",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-20T18:21:12.160395",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
